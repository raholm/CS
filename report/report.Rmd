---
title: "Computational Statistics"
subtitle: Lab 3
author: "Emil K Svensson and Rasmus Holm"
date: '`r Sys.Date()`'
output:
    pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


# Question 1

## 1.1

```{r}
pop <- read.csv2("../data/population.csv", encoding = "latin1")
```

## 1.2

```{r}
cityUnif <- function(data){
    data$prob <- data$Population / sum(data$Population)
    data$cumprob <- cumsum(data$prob)
    return(which.min(data$cumprob < runif(1)))
}
```

# 1.3

```{r}
n <- 20

rmpop <- pop
selpop <-  data.frame()

set.seed(123456)

for (i in 1:n) {
    popidx <- cityUnif(rmpop)
    selpop <- rbind(selpop, rmpop[popidx,])
    rmpop <- rmpop[-popidx,]
}
```


## 1.4

```{r}
print(paste(as.character(selpop$Municipality),":",selpop$Population))
```

As seen in the output above, we in general select cities with large populations. This is logical since the addition of cities is based on the population, i.e. highly populated cities are more likely to be added, thus it is less likely to add smaller cities in the output.

## 1.5

```{r}
library(ggplot2)

true_pop <- data.frame(pop=pop$Population)
picked_pop <- data.frame(pop=selpop$Population)

true_pop$data <- "True"
picked_pop$data <- "Selected"

plot_data <- rbind(true_pop, picked_pop)

ggplot(plot_data, aes(pop, fill=data)) +
    geom_histogram(color="black", binwidth=10000) +
    facet_grid(data ~ ., scales="free_y") +
    xlab("Population") + ylab("Count") +
    labs(fill="Data")
```

In the histograms above the width of a staple is 10000 and we can see that in the data set most cities are $\leq 50000$ in population and the cities we selected are all over the place in terms of the size. We would expect to pick larger cities but since there are so many mid-sized cities in the data set it is also likely to get some of those, that is why we get such a mix of sizes.

\newpage

# Question 2

## 2.1

We have

\begin{align*}
p(x) = \frac{1}{2} e^{-|x|}.
\end{align*}

For $X \geq 0$

\begin{align*}
F(x) &= \int_{-\infty}^{0} \frac{1}{2} e^x dx + \int_{0}^{x} \frac{1}{2} e^{-x} dx \\
     &= \frac{1}{2} \left[ e^x \right]_{-\infty}^{0} - \frac{1}{2} \left[ e^{-x} \right]_{0}^{x} \\
     &= 1 - \frac{1}{2} e^{-x} \\
     &\Downarrow \\
x    &= -\log(2 - 2y) \text{, } y \sim U(0.5, 1)
\end{align*}


For $X < 0$

\begin{align*}
F(x) &= \int_{-\infty}^{x} \frac{1}{2} e^x dx \\
     &= \frac{1}{2} \left[ e^x \right]_{-\infty}^{x} \\
     &= \frac{1}{2} e^x \\
     &\Downarrow \\
x    &= \log(2y) \text{, } y \sim U(0, 0.5)
\end{align*}

```{r}
invLap <- function(){
    rng <- runif(1)

    if(rng > 0.5) {
        return(-log(2 - 2 * rng) )
    } else {
        return(log(2 * rng))
    }
}
```


```{r}
plotdata <- sapply(1:10000,FUN = function(x) invLap())

hist(plotdata, prob = TRUE, ylim = c(0,0.5), xlim = c(-6,6),
     xlab = "x", main = "Laplace distribution ~(0,1)",
     sub = "  Inverse CDF method", breaks = 20)
lines(density(plotdata), col = "red", lwd = 2)
```

Yes, the histogram and the density-curve we included seems reasonable compared to the normal shape of a Laplace distribution.

## 2.2

```{r}
DE01 <- function(x) {
    (1 / 2) * exp(-abs(x))
}

ar <- function(c) {
  rej <- 0

  generated <- FALSE
  x <- c()

  while(!generated){
      y <- invLap() # Y ~ f_y
      u <- runif(1) # U(0,1)

      fx <- dnorm(y, mean = 0, sd = 1) # fx(y)
      fy <- DE01(y) #f_y

      if(u < fx / (c * fy)){
          return(c(y, rej))  #alt. set x <- y and generated = TRUE, to end the loop
      }

      rej <- rej + 1
  }
}
```

### Choosing the appropriate c

```{r, eval=FALSE, echo=FALSE}
set.seed(123456)
n <- 100000
lap <- sapply(1:n,FUN = function(x) invLap())
nor <- rnorm(n, 0, 1)

plot(x = 0, y = 0, col ="white", xlim = c(-6,6),
     ylim = c(-0.01, 0.5), xlab = "x",
     ylab = "Density" )
lines(density(lap), col = "red", lwd = 2)
lines(density(nor), col = "blue", lwd = 2)
```

We know that

\begin{align*}
f_y(x) &= \frac{1}{2} e^{-x} \\
f_x(x) &= \frac{1}{\sqrt{2 \pi}} e^{-\frac{x^2}{2}}.
\end{align*}


For $x \geq 0$ since it is symmetric

\begin{align*}
c f_y(x) \geq f_x(x) \\
c \geq \frac{2}{\sqrt{2 \pi}} e^{-\frac{x^2}{2} + x}.
\end{align*}

To get the minimum $c$ required we have to maximize $e^{-\frac{x^2}{2} + x}$ which is the same as to maximize
$-\frac{x^2}{2} + x$.

\begin{align*}
g(x) &= -\frac{x^2}{2} + x \\
g^{\prime}(x) &= 1 - x \\
1 - x &= 0 \Rightarrow x = 1
\end{align*}

This gives us that

\begin{align*}
c &= \frac{2}{\sqrt{2 \pi}} e^{-\frac{1^2}{2} + 1} \\
  &= \frac{2}{\sqrt{2 \pi}} \sqrt{e} \\
  & \approx 1.32
\end{align*}

### Simulation and comparison

```{r}
ARsim <- data.frame(rn = 1, rej = 1)
c <- 1.32

for(i in 1:2000){
    ARsim[i,] <- ar(c=c)
}
```

```{r}
plot(x = 0, y = 0, col ="white", xlim = c(-5,5),
     ylim = c(-0.01,0.5), xlab = "x", ylab = "Density" )
lines(density(ARsim$rn), col="red", lwd=2)
lines(density(nor), col="blue", lwd=2)
```

We can see from the plot above that the simulated distribution (red) is fairly similar to the normal distribution (blue).

```{r}
par(mfrow = c(1,2))
hist(ARsim$rn, main = "Simulated Normal", xlab = "x",
     col = "orange",  breaks = 30)

set.seed(123456)
hist(rnorm(2000,0,1), main = "Normal drawn from rnorm", xlab = "x",
     col = "lightgreen", breaks = 30 )
```

```{r, echo=FALSE}
par(mfrow = c(1,1))
```

```{r, echo=FALSE}
options(digits=3)
```

The simulated one is a bit more rough around the edges and more narrow in the center in comparison to the histogram drawn from the rnorm.

The expected rejection rate is $\frac{n(M - 1)}{nM}$ since the expected number of iterations before a accepted value is $M$ so intuitively the number of rejections is $M - 1$. n here is only the number of values that we use, but these cancel each other out so they are not necessary but we left them here for clarity. The theoretical rejection rate would be `r (c - 1) / c` and what we empirically found was `r sum(ARsim$rej) / (sum(ARsim$rej) + 2000)` which is reasonably close.
