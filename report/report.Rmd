---
title: "Computational Statistics"
subtitle: Lab 5
author: "Emil K Svensson and Rasmus Holm"
date: '`r Sys.Date()`'
output:
    pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Question 1

## 1.1

```{r}
library(ggplot2)

lottery <- read.csv2("../data/lottery.csv")

q11 <- ggplot(lottery, aes(x = Day_of_year, y = Draft_No)) + geom_point()
plot(q11)

data <- data.frame(x=lottery$Day_of_year, y=lottery$Draft_No)
```

The data looks random although there might be some sort of skewness in the right side of the graph were there are a lacking some observations and therefore having a lower probability of getting selected.

## 1.2

```{r}
loessfit <- loess(y ~ x, data=data)
data$pred <- predict(loessfit, data$x)

q12 <- q11 + geom_path(data = data, aes(x=x, y=pred), col = "red")
plot(q12)
```

The fit (line) doesn't seem straight and seems to have a decreasing trend which would support previous statements of people born on a days later on in a year has a lower probability of beeing selected.

## 1.3

```{r}
teststat <- function(model) {
    function(data) {
        xa <- data$x[which.min(data$y)]
        xb <- data$x[which.max(data$y)]

        fit <- model(y ~ x, data)

        ya <- predict(fit, xa)
        yb <- predict(fit, xb)

        (yb - ya) / (xb - xa)
    }
}

teststat_boot <- function(data, idx, stat) {
    data <- data[idx,]
    stat(data)
}
```

```{r}
library(boot)

B <- 2000

set.seed(123456)
npboot <- boot(data=data, statistic=teststat_boot, R=B, stat=teststat(model=loess))
pvalue <- sum(npboot$t > 0) / B
pvalue
```

```{r}
hist(npboot$t, xlim = c(-1,0.7), breaks = 50,
     main = "Histogram for bootstrap t-values", xlab ="T")
abline(v=pvalue, col = "red", lwd = 2)
abline(v=mean(npboot$t), col="blue", lwd=2)
```

In this task we use the 2000 calculated T-values from the bootstrap and use these as the T-distribution and thus the share of T-values over the null-hypothies will be the p-value. Since it is implied that if the fit of the loess function has a decreasing pattern it will generate a negative T-value, since this is idicated in the previous graph. Subsequently the test was formulated as follows $H_o: \mu >= 0 \\ H_a: \mu <0$ 

The p-value was calculated to `r pvalue` and we can't reject the nullhypothesis at a 0.05 alpha-level. We conclude Ho and that the lottery is random.

## 1.4

```{r}
teststat_permutation<- function(data, B, stat) {
    n <- nrow(data)
    statistics <- rep(0, B)
    newdata <- data.frame(x=data$x, y=sample(data$y, n))

    for (b in 1:B) {
        statistics[b] <- stat(newdata)
        newdata$y <- sample(data$y, n)
    }

    sum(abs(statistics) >= abs(stat(data))) / B
}

set.seed(123456)
pvalue2 <- teststat_permutation(data, B, teststat(loess))
pvalue2
```

In permutation tests we compare the original t-statistic with T-statistics where the Draft No. are shuffled on random. In this sense if these diverge much from the original T-statistic the lottory is random. If they are similar we will get a lower p-value since the quota will be smaller.

In this case we conclude Ho in this test as well, and asert that the lottery is random.

## 1.5

```{r}
genranddata <- function(x, alpha) {
    data.frame(x=x, y=pmax(0, pmin(alpha * x + rnorm(length(x), mean=183, sd=10), 366)))
}

alphas <- seq(0.1, 10, by=0.1)
pvalues <- rep(0, length(alphas))

set.seed(123456)

for (i in 1:length(alphas)) {
    newdata <- genranddata(data$x, alphas[i])
    pvalues[i] <- teststat_permutation(newdata, 200, teststat(loess))
}

plot(pvalues)
```

```{r}
print(sum(pvalues <= 0.05))
```
Since we reject Ho in every test the power will be 1 becaus we can't have any Type-2 errors when we reject everything. This isn't strange since the test-data is obviously non-random and gives us an indication that the test-statistic is a good one.

\newpage

## Question 2

## 2.1

```{r}
price <- read.csv("../data/prices1.csv", sep=";")
mean(price$Price)
```

```{r}
hist(price$Price)
```

Looks like a Gamma distribution.

## 2.2

```{r}
bootmean <- function(data,ind){
  data <- data[ind]
  mean(data)
}

B <- 2000

estmean <- boot(data = price$Price,bootmean, R = B)

2 * estmean$t0 - mean(estmean$t)

mean(estmean$t- estmean$t0)

sum((estmean$t - mean(estmean$t))^2) / (B - 1)
cibo <- boot.ci(estmean)
print(cibo)
```


```{r}
hist(estmean$t, xlim = c(950,1300), main = "Bootstrap estimation of mean", xlab = "")
abline(v=cibo$normal[2:3], col = "blue", lwd = 2)
abline(v=cibo$basic[4:5], col = "red", lwd = 2)
abline(v=cibo$percent[4:5], col = "orange", lwd = 2)
abline(v=cibo$bca[4:5], col = "purple", lwd = 2)
legend(y=400, x=1200, c("normal","basic","percent","bca"), lty = c(1,1),
       col=c("blue", "red", "orange", "purple"))
```

## 2.3

```{r}
jackknife <- function(data,B,tstat){
  stopifnot(B >= 0 && B <= length(data))
  est <- rep(1, times = B)
  for(i in 1:B){

    est[i] <- tstat(data[-i])

  }
  return(est)
}

jackest <- jackknife(price$Price, B = length(price$Price), tstat = mean)

mean(jackest)

n <- length(price$Price)

tstar <- n*mean(price$Price)-(n-1)*jackest
JT <- mean(tstar)
jackvar <- (1 / (n * (n - 1))) * sum((tstar - JT)^2)

lowerci <- JT - 1.96 * sqrt(jackvar)
upperci <- JT + 1.96 * sqrt(jackvar)

hist(jackest,breaks = 10, xlim = c(1000,1180), xlab = "" ,main = "Jackknife estimate")
abline(v=lowerci, col ="blue", lwd = 3)
abline(v=upperci, col ="blue", lwd = 3)
```

## 2.4

```{r, warning=FALSE}
library(knitr)
options(digits=2)

cimatrix <- rbind(
  c("Normal", cibo$normal[2], mean(estmean$t), cibo$normal[3]),
  c("Percent", cibo$percent[4], mean(estmean$t), cibo$percent[5]),
  c("Basic", cibo$basic[4], mean(estmean$t), cibo$basic[5]),
  c("Bca", cibo$bca[4], mean(estmean$t), cibo$bca[5]),
  c("Jackknife", lowerci, mean(jackest), upperci)
)
colnames(cimatrix) <- c("Name", "Lower", "Mean", "Upper")

cimatrix[, 2:4] <- round(as.numeric(cimatrix[, 2:4]))

kable(cimatrix)
```

The normal, percent  and the basic along with the Jackknife confidence intervals are moving in a similar range with only small differences inbetween them. The only one sticking out is the bca CI with overall higher estimations than the rest.
